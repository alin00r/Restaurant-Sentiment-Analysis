# -*- coding: utf-8 -*-
"""restaurant sentiment analysis arabic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KpoGMhmcOHVUyW6_Oau-G2vQ1FlrkX9A
"""

!pip install keras
!pip install tensorflow
!pip install arabic_reshaper
!pip install python-bidi
!pip install pad_sequences
!pip install keras_preprocessing

import os
import re
import pickle
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from bidi.algorithm import get_display
from arabic_reshaper import arabic_reshaper
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.backend import clear_session
from keras.models import Sequential
from keras import layers

pd.set_option('display.max_colwidth', None)

seed = 42

def clean_text(text):
    # ref: https://github.com/bakrianoo/aravec
    search = ["أ","إ","آ","ة","_","-","/",".","،"," و "," يا ",'"',"ـ","'","ى",
              "\\",'\n', '\t','&quot;','?','؟','!']
    replace = ["ا","ا","ا","ه"," "," ","","",""," و"," يا",
               "","","","ي","",' ', ' ',' ',' ? ',' ؟ ', ' ! ']

    tashkeel = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
    text = re.sub(tashkeel,"", text)

    longation = re.compile(r'(.)\1+')
    subst = r"\1\1"
    text = re.sub(longation, subst, text)

    text = re.sub(r"[^\w\s]", '', text)
    text = re.sub(r"[a-zA-Z]", '', text)
    text = re.sub(r"\d+", ' ', text)
    text = re.sub(r"\n+", ' ', text)
    text = re.sub(r"\t+", ' ', text)
    text = re.sub(r"\r+", ' ', text)
    text = re.sub(r"\s+", ' ', text)
    text = text.replace('وو', 'و')
    text = text.replace('يي', 'ي')
    text = text.replace('اا', 'ا')

    for i in range(0, len(search)):
        text = text.replace(search[i], replace[i])

    text = text.strip()

    return text

data = pd.read_csv('Arabic_resturant_reviews.csv')
data.head()

data.shape

data = data[data.label.isin([-1, 1])]

data.shape

data['cleaned_text'] = data.review.apply(clean_text)

data.shape

data = data[data.cleaned_text != ""]
data.head(3)

min_sample = data.groupby(['label']).count().review.min()
min_sample

input_data = pd.concat([data[data.label == 1].head(min_sample),
                        data[data.label == -1].head(min_sample)])
input_data

input_data.groupby(['label']).count()

X = input_data.cleaned_text.values

Y = np.asarray(input_data.label.values).astype('float32')
Y = Y.clip(0, 1)

num_words = 10000
tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)

vocab_size = len(tokenizer.word_index) + 1
print("vocab size:", vocab_size)

maxlen = 300
X = pad_sequences(X, padding='post', maxlen=maxlen)

with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

X_train, X_test, label_train, label_test = train_test_split(X, Y, test_size=0.3,
                                                            random_state=seed)

print("Training:", len(X_train), len(label_train))
print("Testing: ", len(X_test), len(label_test))

embedding_dim = 100
dropout = 0.5
opt = 'adam'
clear_session()

model = Sequential()
model.add(layers.Embedding(input_dim=num_words,
                           output_dim=embedding_dim,
                           input_length=maxlen))
model.add(layers.Bidirectional(layers.LSTM(100, dropout=dropout,
                                           recurrent_dropout=dropout,
                                           return_sequences=True)))
model.add(layers.GlobalMaxPool1D())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(dropout))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(dropout))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dropout(dropout))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer=opt,
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.summary()

history = model.fit(X_train, label_train,
                    epochs=4,
                    verbose=True,
                    validation_data=(X_test, label_test),
                    batch_size=64)
loss, accuracy = model.evaluate(X_train, label_train, verbose=True)
print("Training Accuracy: {:.4f}".format(accuracy))
loss_val, accuracy_val = model.evaluate(X_test, label_test, verbose=True)
print("Testing Accuracy:  {:.4f}".format(accuracy_val))

blind_sample = 200
blind_test = pd.concat([data[data.label == 1].tail(blind_sample),
                        data[data.label == -1].tail(blind_sample)])
blind_test

X_blind = blind_test.cleaned_text.values
Y_blind = np.asarray(blind_test.label.values).astype('float32')
Y_blind = Y_blind.clip(0, 1)

X_blind = tokenizer.texts_to_sequences(X_blind)
X_blind = pad_sequences(X_blind, padding='post', maxlen=maxlen)
X_blind

pred_blind = model.predict(X_blind, verbose=True)

df_blind = pd.DataFrame({'REAL': Y_blind,
                         'PRED': pred_blind.reshape(pred_blind.shape[0],),
                         'TEXT': blind_test.cleaned_text})
df_blind = df_blind.reset_index()[['REAL', 'PRED', 'TEXT']]
df_blind.PRED = df_blind.PRED.round()
error_records = df_blind[df_blind.REAL != df_blind.PRED]

print("Number of misclassified reviews: {} out of {}".format(error_records.shape[0], df_blind.shape[0]))
print("Blind Test Accuracy:  {:.4f}".format(accuracy_score(df_blind.REAL, df_blind.PRED)))

df_blind.sample(n=100)

with open('model_acc{}.json'.format(round(accuracy_val, 4)), 'w') as f:
    f.write(model.to_json())
    f.close()

model.save('resturent_sentiment_analysis_arabic.h5'.format(round(accuracy_val, 4)))

import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle

# Load tokenizer
with open('tokenizer.pkl', 'rb') as f:
    tokenizer = pickle.load(f)

# Load model
model = load_model('resturent_sentiment_analysis_arabic.h5')

# Maximum length for padding sequences
maxlen = 300

def preprocess_text(text):
    # Clean and preprocess the text
    cleaned_text = clean_text(text)
    tokenized_text = tokenizer.texts_to_sequences([cleaned_text])
    padded_text = pad_sequences(tokenized_text, padding='post', maxlen=maxlen)
    return padded_text

def classify_text(text):
    # Preprocess text
    preprocessed_text = preprocess_text(text)

    # Predict sentiment
    prediction = model.predict(preprocessed_text)
    sentiment = "ايجابي" if prediction > 0.5 else "سلبي"

    return sentiment

if __name__ == "__main__":
    # Input text
    text = input("Enter the text to classify: ")

    # Classify text
    result = classify_text(text)
    print(f"The sentiment of the text is: {result}")

from joblib import dump

# Save tokenizer
dump(tokenizer, 'tokenizer.joblib')

# Input text
    text = input("Enter the text to classify: ")

    # Classify text
    result = classify_text(text)
    print(f"The sentiment of the text is: {result}")

positive = get_display(arabic_reshaper.reshape('تعليقات ايجابية'))
negative = get_display(arabic_reshaper.reshape('تعليقات سلبية'))

# Your class distribution data
class_distribution = data['label'].value_counts()

# Define mapping from numerical labels to Arabic strings
label_mapping = {1: positive, -1: negative}

# Map numerical labels to Arabic strings
class_distribution.index = class_distribution.index.map(label_mapping)

# Set seaborn style
sns.set(style="whitegrid", font_scale=1.2)

# Plotting
plt.figure(figsize=(8, 6))  # Adjust figure size as needed
sns.barplot(x=class_distribution.index, y=class_distribution.values, palette=["#1f77b4", "#d62728"])
plt.title('Distribution of Classes', fontsize=18)
plt.xlabel('Class', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(fontsize=12)  # Adjust x-axis tick font size
plt.yticks(fontsize=12)  # Adjust y-axis tick font size
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

from wordcloud import WordCloud
classes = data['label'].unique()
fig, axs = plt.subplots(1, len(classes), figsize=(15, 7))

for i, label in enumerate(classes):
    # Filter reviews based on class label
    reviews = ' '.join(data[data['label'] == label]['review'])

    # Generate word cloud
    wordcloud = WordCloud(font_path='DejaVuSans-Bold.ttf',
                          background_color='white',
                          width=400,
                          height=400,
                          max_words=200,
                          contour_color='steelblue').generate(reviews)

    # Display the word cloud
    axs[i].imshow(wordcloud, interpolation='bilinear')
    title = negative if label == 0 else positive
    axs[i].set_title(title)
    axs[i].axis('off')

plt.tight_layout()
plt.show()

# Input text
    text = input("Enter the text to classify: ")

    # Classify text
    result = classify_text(text)
    print(f"The sentiment of the text is: {result}")

# Input text
    text = input("Enter the text to classify: ")

    # Classify text
    result = classify_text(text)
    print(f"The sentiment of the text is: {result}")

# Input text
    text = input("Enter the text to classify: ")

    # Classify text
    result = classify_text(text)
    print(f"The sentiment of the text is: {result}")

# Input text
    text = input("Enter the text to classify: ")

    # Classify text
    result = classify_text(text)
    print(f"The sentiment of the text is: {result}")

# Input text
    text = input("Enter the text to classify: ")

    # Classify text
    result = classify_text(text)
    print(f"The sentiment of the text is: {result}")